{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(\"/home/luisca1985/spark-2.4.6-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Mi programa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mi programa</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Mi programa>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"ejemplo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La COVID-19 (Acerca de este sonido pronunciación (?·i); acrónimo del inglés coronavirus disease 2019), también conocida como enfermedad por coronavirus e incorrectamente como neumonía por coronavirus, es una enfermedad infecciosa causada por el virus SARS-CoV-2.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = lines.filter(lambda line: \"coronavirus\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[24] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La COVID-19 (Acerca de este sonido pronunciación (?·i); acrónimo del inglés coronavirus disease 2019), también conocida como enfermedad por coronavirus e incorrectamente como neumonía por coronavirus, es una enfermedad infecciosa causada por el virus SARS-CoV-2.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 18, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-46-b7d8b1f82f6f>\", line 1, in <lambda>\nTypeError: 'in <string>' requires string as left operand, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-46-b7d8b1f82f6f>\", line 1, in <lambda>\nTypeError: 'in <string>' requires string as left operand, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b7d8b1f82f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 18, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-46-b7d8b1f82f6f>\", line 1, in <lambda>\nTypeError: 'in <string>' requires string as left operand, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/luisca1985/spark-2.4.6-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-46-b7d8b1f82f6f>\", line 1, in <lambda>\nTypeError: 'in <string>' requires string as left operand, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "lines.filter(lambda x: int() in x).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.filter(lambda x: any(i.isdigit() for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La COVID-19 (Acerca de este sonido pronunciación (?·i); acrónimo del inglés coronavirus disease 2019), también conocida como enfermedad por coronavirus e incorrectamente como neumonía por coronavirus, es una enfermedad infecciosa causada por el virus SARS-CoV-2.',\n",
       " 'Produce síntomas similares a los de la gripe, entre los que se incluyen fiebre, tos seca, disnea, mialgia y fatiga. En casos graves se caracteriza por producir neumonía, síndrome de dificultad respiratoria aguda, sepsis y choque séptico que conduce a cerca de 3,75 % de los infectados a la muerte según la OMS. No existe tratamiento específico; las medidas terapéuticas principales consisten en aliviar los síntomas y mantener las funciones vitales.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.filter(lambda x: any(i.isdigit() for i in x)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.filter(lambda x: not any(i.isdigit() for i in x)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeros = lines.filter(lambda x: any(i.isdigit() for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[35] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeros.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"latin1\").option(\"sep\", \";\").load(\"covid19bogota_23082020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+--------------------+-----------------------+----+---------------------------+----+------------+---------+----------+\n",
      "|Fecha de inicio de síntomas|Fecha de diagnóstico|Ciudad de residencia|Localidad de residencia|Edad|Unidad de medida de la edad|Sexo|Tipo de caso|Ubicación|    Estado|\n",
      "+---------------------------+--------------------+--------------------+-----------------------+----+---------------------------+----+------------+---------+----------+\n",
      "|                 26/02/2020|           6/03/2020|              Bogotá|                Usaquén|  19|                          1|   F|   Importado|     Casa|Recuperado|\n",
      "|                  4/03/2020|          10/03/2020|              Bogotá|               Engativá|  22|                          1|   F|   Importado|     Casa|Recuperado|\n",
      "+---------------------------+--------------------+--------------------+-----------------------+----+---------------------------+----+------------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Fecha de inicio de síntomas='26/02/2020', Fecha de diagnóstico='6/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Usaquén', Edad=19, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado'),\n",
       " Row(Fecha de inicio de síntomas='4/03/2020', Fecha de diagnóstico='10/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Engativá', Edad=22, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188220"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(fraction=0.001,withReplacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"diagnostico\", to_date(df2[\"Fecha de diagnóstico\"],\"dd/MM/yyyy\")).withColumn(\"sintomas\", to_date(df2[\"Fecha de inicio de síntomas\"],\"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Fecha de inicio de síntomas='18/03/2020', Fecha de diagnóstico='23/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Chapinero', Edad=60, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 3, 23), sintomas=datetime.date(2020, 3, 18)),\n",
       " Row(Fecha de inicio de síntomas='1/04/2020', Fecha de diagnóstico='4/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Usaquén', Edad=61, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 4), sintomas=datetime.date(2020, 4, 1)),\n",
       " Row(Fecha de inicio de síntomas='26/03/2020', Fecha de diagnóstico='6/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Kennedy', Edad=67, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 6), sintomas=datetime.date(2020, 3, 26)),\n",
       " Row(Fecha de inicio de síntomas='8/04/2020', Fecha de diagnóstico='18/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Kennedy', Edad=51, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 18), sintomas=datetime.date(2020, 4, 8)),\n",
       " Row(Fecha de inicio de síntomas='26/04/2020', Fecha de diagnóstico='28/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='San Cristóbal', Edad=23, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 28), sintomas=datetime.date(2020, 4, 26))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.na.drop(subset = [\"Edad\",\"Localidad de residencia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.filter(\"Edad is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fecha de inicio de síntomas: string (nullable = true)\n",
      " |-- Fecha de diagnóstico: string (nullable = true)\n",
      " |-- Ciudad de residencia: string (nullable = true)\n",
      " |-- Localidad de residencia: string (nullable = true)\n",
      " |-- Edad: integer (nullable = true)\n",
      " |-- Unidad de medida de la edad: integer (nullable = true)\n",
      " |-- Sexo: string (nullable = true)\n",
      " |-- Tipo de caso: string (nullable = true)\n",
      " |-- Ubicación: string (nullable = true)\n",
      " |-- Estado: string (nullable = true)\n",
      " |-- diagnostico: date (nullable = true)\n",
      " |-- sintomas: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "media = np.mean(df3.select(\"Edad\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.88944723618091"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones básicas en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Edad=60), Row(Edad=61), Row(Edad=67), Row(Edad=51), Row(Edad=23)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select('Edad').filter(\"Estado == 'Fallecido' or Estado == 'Recuperado'\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Fecha de inicio de síntomas='18/03/2020', Fecha de diagnóstico='23/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Chapinero', Edad=60, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 3, 23), sintomas=datetime.date(2020, 3, 18)),\n",
       " Row(Fecha de inicio de síntomas='1/04/2020', Fecha de diagnóstico='4/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Usaquén', Edad=61, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 4), sintomas=datetime.date(2020, 4, 1)),\n",
       " Row(Fecha de inicio de síntomas='26/03/2020', Fecha de diagnóstico='6/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Kennedy', Edad=67, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 6), sintomas=datetime.date(2020, 3, 26)),\n",
       " Row(Fecha de inicio de síntomas='8/04/2020', Fecha de diagnóstico='18/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Kennedy', Edad=51, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='Relacionado', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 18), sintomas=datetime.date(2020, 4, 8)),\n",
       " Row(Fecha de inicio de síntomas='26/04/2020', Fecha de diagnóstico='28/04/2020', Ciudad de residencia='Bogotá', Localidad de residencia='San Cristóbal', Edad=23, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio', Ubicación='Casa', Estado='Recuperado', diagnostico=datetime.date(2020, 4, 28), sintomas=datetime.date(2020, 4, 26))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.filter(\"Estado == 'Fallecido' or Estado == 'Recuperado'\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Edad=60)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select('Edad').filter(\"Estado == 'Fallecido' or Estado == 'Recuperado'\").take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([445.65543799]),\n",
       " array([488.87654352]),\n",
       " array([790.20317669]),\n",
       " array([146.66548825]),\n",
       " array([252.47453347]),\n",
       " array([171.88659377]),\n",
       " array([192.91674453]),\n",
       " array([65.78106613]),\n",
       " array([4.45443297]),\n",
       " array([4.45443297])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select('Edad').rdd.map(lambda x: (x-media)**2).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Estado|count|\n",
      "+--------------------+-----+\n",
      "|           Fallecido|    6|\n",
      "|            Moderado|    6|\n",
      "|Leve             ...|    3|\n",
      "|          Recuperado|  125|\n",
      "|                Leve|   58|\n",
      "|               Grave|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Estado\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Sexo|         avg(Edad)|\n",
      "+----+------------------+\n",
      "|   F|38.211009174311926|\n",
      "|   M| 39.71111111111111|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Sexo\").mean(\"Edad\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Estado='Recuperado'),\n",
       " Row(Estado='Moderado'),\n",
       " Row(Estado='Grave'),\n",
       " Row(Estado='Fallecido'),\n",
       " Row(Estado='Leve'),\n",
       " Row(Estado='Leve                                                                                                                                                                                                    ')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select(\"Estado\").rdd.distinct().take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select(\"Estado\").rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Fecha de inicio de síntomas='21/08/2020', Fecha de diagnóstico='22/08/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Ciudad Bolívar', Edad=32, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio                                                                                                                                                                                              ', Ubicación='Casa                                                                                                                                                                                                    ', Estado='Leve                                                                                                                                                                                                    ', diagnostico=datetime.date(2020, 8, 22), sintomas=datetime.date(2020, 8, 21)),\n",
       " Row(Fecha de inicio de síntomas='18/08/2020', Fecha de diagnóstico='21/08/2020', Ciudad de residencia='Sin dato', Localidad de residencia='Sin dato', Edad=47, Unidad de medida de la edad=1, Sexo='M', Tipo de caso='En estudio                                                                                                                                                                                              ', Ubicación='Casa                                                                                                                                                                                                    ', Estado='Leve                                                                                                                                                                                                    ', diagnostico=datetime.date(2020, 8, 21), sintomas=datetime.date(2020, 8, 18)),\n",
       " Row(Fecha de inicio de síntomas='10/08/2020', Fecha de diagnóstico='21/08/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Kennedy', Edad=15, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio                                                                                                                                                                                              ', Ubicación='Casa                                                                                                                                                                                                    ', Estado='Leve                                                                                                                                                                                                    ', diagnostico=datetime.date(2020, 8, 21), sintomas=datetime.date(2020, 8, 10)),\n",
       " Row(Fecha de inicio de síntomas='9/08/2020', Fecha de diagnóstico='20/08/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Ciudad Bolívar', Edad=58, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio', Ubicación='Hospital', Estado='Moderado', diagnostico=datetime.date(2020, 8, 20), sintomas=datetime.date(2020, 8, 9)),\n",
       " Row(Fecha de inicio de síntomas='30/07/2020', Fecha de diagnóstico='20/08/2020', Ciudad de residencia='Bogotá', Localidad de residencia='La Candelaria', Edad=46, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='En estudio                                                                                                                                                                                              ', Ubicación='Casa', Estado='Leve', diagnostico=datetime.date(2020, 8, 20), sintomas=datetime.date(2020, 7, 30))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.orderBy(df3.diagnostico.desc()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones básicas en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"latin1\").option(\"sep\", \";\").load(\"covid19bogota_23082020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.sample(fraction=0.01, withReplacement=False)\n",
    "df4 = df4.withColumn(\"diagnostico\", to_timestamp(\"Fecha de diagnóstico\",\"dd/MM/yyyy\")).withColumn(\"sintomas\", to_timestamp(\"Fecha de inicio de síntomas\",\"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.withColumn(\"duracion\", (unix_timestamp(\"diagnostico\") - unix_timestamp(\"sintomas\"))/(60*60*24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.na.drop()\n",
    "df5 = df5.filter(\"Edad is not NULL\")\n",
    "df5 = df5.filter(\"Estado == 'Fallecido' or Estado == 'Recuperado'\")\n",
    "df5 = df5.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+\n",
      "|summary|              Edad|        duracion|\n",
      "+-------+------------------+----------------+\n",
      "|  count|               981|             981|\n",
      "|   mean|40.103975535168196|11.1855249745158|\n",
      "| stddev|18.458707211884985|8.11588480562669|\n",
      "|    min|                 1|             0.0|\n",
      "|    max|                97|            60.0|\n",
      "+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(\"Edad\",\"duracion\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {Row(Estado='Recuperado'): 864, Row(Estado='Fallecido'): 56})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.select(\"Estado\").rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Estado='Recuperado')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.select(\"Estado\").rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5.select(\"Estado\").rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Edad_Estado='69', Fallecido=4, Recuperado=7),\n",
       " Row(Edad_Estado='5', Fallecido=0, Recuperado=4),\n",
       " Row(Edad_Estado='10', Fallecido=0, Recuperado=6),\n",
       " Row(Edad_Estado='56', Fallecido=0, Recuperado=11),\n",
       " Row(Edad_Estado='42', Fallecido=0, Recuperado=16),\n",
       " Row(Edad_Estado='24', Fallecido=0, Recuperado=28),\n",
       " Row(Edad_Estado='37', Fallecido=0, Recuperado=24),\n",
       " Row(Edad_Estado='25', Fallecido=0, Recuperado=21),\n",
       " Row(Edad_Estado='52', Fallecido=0, Recuperado=15),\n",
       " Row(Edad_Estado='14', Fallecido=0, Recuperado=5)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.crosstab(\"Edad\",\"Estado\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones numéricas con RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = sc.parallelize(range(1,10000))\n",
    "lista.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = df5.select(mean(col(\"Edad\"))).collect()\n",
    "std = df5.select(stddev(col(\"Edad\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stddev_samp(Edad)=18.692128194332557)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.692128194332557"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5.withColumn('edad_std',(df5[\"Edad\"]-media[0][0])/std[0][0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('a', 5), ('b', 6), ('c', 7), ('d',8)])\n",
    "y = sc.parallelize([('a', 1), ('a', 2), ('c', 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', (7, 3)), ('a', (5, 1)), ('a', (5, 2))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', (3, 7)), ('a', (1, 5)), ('a', (2, 5))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.join(x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', (3, 7)), ('a', (1, 5)), ('a', (2, 5))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.leftOuterJoin(x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (6, None)),\n",
       " ('c', (7, 3)),\n",
       " ('d', (8, None)),\n",
       " ('a', (5, 1)),\n",
       " ('a', (5, 2))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.leftOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', (3, 7)),\n",
       " ('b', (None, 6)),\n",
       " ('d', (None, 8)),\n",
       " ('a', (1, 5)),\n",
       " ('a', (2, 5))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.rightOuterJoin(x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acumuladores. Detectar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('ejemplo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = sc.accumulator(0)\n",
    "sp = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenguajes(linea):\n",
    "    global py, sp\n",
    "    if \"Python\" in linea:\n",
    "        py += 1\n",
    "        if 'Spark' in linea:\n",
    "            sp += 1\n",
    "        return True\n",
    "    if 'Spark' in linea:\n",
    "        sp += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = lines.filter(lenguajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir funciones map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sc.parallelize(df5.select('Edad').rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[89] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapfunction = A.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=76), 1),\n",
       " (Row(Edad=37), 1),\n",
       " (Row(Edad=11), 1),\n",
       " (Row(Edad=45), 1),\n",
       " (Row(Edad=43), 1),\n",
       " (Row(Edad=43), 1),\n",
       " (Row(Edad=21), 1),\n",
       " (Row(Edad=12), 1),\n",
       " (Row(Edad=70), 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapfunction.collect()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    if x[0] in range(10,30):\n",
    "        return((x,2))\n",
    "    elif x[0] == 35:\n",
    "        return((x,3))\n",
    "    else:\n",
    "        return((x,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapfunction2 = A.map(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=76), 1),\n",
       " (Row(Edad=37), 1),\n",
       " (Row(Edad=11), 2),\n",
       " (Row(Edad=45), 1),\n",
       " (Row(Edad=43), 1),\n",
       " (Row(Edad=43), 1),\n",
       " (Row(Edad=21), 2),\n",
       " (Row(Edad=12), 2),\n",
       " (Row(Edad=70), 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapfunction2.collect()[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir funciones reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducefunction = mapfunction.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducefunction2 = mapfunction.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=27), 29),\n",
       " (Row(Edad=76), 7),\n",
       " (Row(Edad=37), 30),\n",
       " (Row(Edad=11), 8),\n",
       " (Row(Edad=45), 8),\n",
       " (Row(Edad=43), 14),\n",
       " (Row(Edad=21), 18),\n",
       " (Row(Edad=12), 7),\n",
       " (Row(Edad=70), 4),\n",
       " (Row(Edad=82), 3)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducefunction.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=27), 29),\n",
       " (Row(Edad=76), 7),\n",
       " (Row(Edad=37), 30),\n",
       " (Row(Edad=11), 8),\n",
       " (Row(Edad=45), 8),\n",
       " (Row(Edad=43), 14),\n",
       " (Row(Edad=21), 18),\n",
       " (Row(Edad=12), 7),\n",
       " (Row(Edad=70), 4),\n",
       " (Row(Edad=82), 3)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducefunction2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=1), 1),\n",
       " (Row(Edad=2), 2),\n",
       " (Row(Edad=3), 3),\n",
       " (Row(Edad=4), 8),\n",
       " (Row(Edad=5), 4),\n",
       " (Row(Edad=6), 3),\n",
       " (Row(Edad=7), 5),\n",
       " (Row(Edad=8), 4),\n",
       " (Row(Edad=9), 4),\n",
       " (Row(Edad=10), 8)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducefunction.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(Edad=37), 30),\n",
       " (Row(Edad=27), 29),\n",
       " (Row(Edad=25), 29),\n",
       " (Row(Edad=42), 27),\n",
       " (Row(Edad=40), 27),\n",
       " (Row(Edad=26), 25),\n",
       " (Row(Edad=38), 25),\n",
       " (Row(Edad=30), 25),\n",
       " (Row(Edad=29), 24),\n",
       " (Row(Edad=28), 24)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducefunction.sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos MapReduce en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('ejemplo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = sc.accumulator(0)\n",
    "sp = sc.accumulator(0)\n",
    "\n",
    "def lenguajes(linea):\n",
    "    global py, sp\n",
    "    if \"Python\" in linea:\n",
    "        py += 1\n",
    "        if \"Spark\" in linea:\n",
    "            sp += 1\n",
    "        return True\n",
    "    elif \"Spark\" in linea:\n",
    "        sp += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "valores = lines.filter(lenguajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=2, value=68>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=3, value=249>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcionmap = valores.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "contarvalores = funcionmap.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contarvalores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Learn Python for Data Science With DataCamp', 2),\n",
       " ('You might already know Apache Spark as a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. It’s well-known for its speed, ease of use, generality and the ability to run virtually everywhere. And even though Spark is one of the most asked tools for data engineers, also data scientists can benefit from Spark when doing exploratory data analysis, feature extraction, supervised learning and model evaluation.',\n",
       "  1),\n",
       " ('Today’s post will introduce you to some basic Spark in Python topics, based on 9 of the most frequently asked questions, such as',\n",
       "  1),\n",
       " ('What language to pick when you’re working with Spark: Python or Scala? What are the benefits of using one over the other?',\n",
       "  1),\n",
       " ('Next, you’ll see how you can work with Spark in Python: locally or via the Jupyter Notebook. You’ll learn how to install Spark and how to run Spark applications with Jupyter notebooks, either by adding PySpark as any other library, by working with a kernel or by running PySpark with Jupyter in Docker containers.',\n",
       "  1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contarvalores.sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenguajes_map(x):\n",
    "    if \"Python\" in x and \"Spark\" in x:\n",
    "        return (\"Count\",(1,1))\n",
    "    elif \"Python\"in x:\n",
    "        return (\"Count\",(1,0))\n",
    "    elif \"Spark\" in x:\n",
    "        return (\"Count\",(0,1))\n",
    "    else:\n",
    "        return (\"Count\",(0,0))\n",
    "    \n",
    "mapfun = lines.map(lenguajes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapfun.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Count', (34, 83))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapfun.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Etl:\n",
    "    def __init__(self, formattype = \"csv\", header = \"true\", inferSchema = \"true\", encoding = \"latin1\", sep = \";\", path_in = \"covid19bogota_23082020.csv\"):\n",
    "        self.format = formattype\n",
    "        self.header = header\n",
    "        self.inferSchema = inferSchema\n",
    "        self.encoding = encoding\n",
    "        self.sep = sep\n",
    "        self.path_in = path_in\n",
    "    \n",
    "    def extract(self):\n",
    "        df = sqlContext.read.format(self.format).option(\"header\", self.header).option(\"inferSchema\", self.inferSchema).option(\"encoding\", self.encoding).option(\"sep\", self.sep).load(self.path_in)\n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df2 = df.groupBy('Estado').agg({'Edad':'avg', 'Edad':'count'})\n",
    "        return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = Etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = prueba.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Fecha de inicio de síntomas='26/02/2020', Fecha de diagnóstico='6/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Usaquén', Edad=19, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado'),\n",
       " Row(Fecha de inicio de síntomas='4/03/2020', Fecha de diagnóstico='10/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Engativá', Edad=22, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado'),\n",
       " Row(Fecha de inicio de síntomas='7/03/2020', Fecha de diagnóstico='10/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Engativá', Edad=28, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado'),\n",
       " Row(Fecha de inicio de síntomas='6/03/2020', Fecha de diagnóstico='12/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Fontibón', Edad=36, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado'),\n",
       " Row(Fecha de inicio de síntomas='6/03/2020', Fecha de diagnóstico='12/03/2020', Ciudad de residencia='Bogotá', Localidad de residencia='Bosa', Edad=42, Unidad de medida de la edad=1, Sexo='F', Tipo de caso='Importado', Ubicación='Casa', Estado='Recuperado')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = prueba.transform(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Estado='Fallecido', count(Edad)=4951),\n",
       " Row(Estado='moderado', count(Edad)=69),\n",
       " Row(Estado='Moderado', count(Edad)=6063),\n",
       " Row(Estado='Leve                                                                                                                                                                                                    ', count(Edad)=4507),\n",
       " Row(Estado='Fallecido No aplica No causa Directa', count(Edad)=343),\n",
       " Row(Estado='Recuperado', count(Edad)=122225),\n",
       " Row(Estado='Leve', count(Edad)=49336),\n",
       " Row(Estado='Grave', count(Edad)=726)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
